#!/usr/bin/env python3
import logging
import pathlib
import sys
from datetime import datetime

import click
from app import cli
from filelock import FileLock, Timeout
from helpers import fetch

from AIPscan import db
from AIPscan.Aggregator import database_helpers
from AIPscan.Aggregator.task_helpers import create_numbered_subdirs
from AIPscan.models import StorageService
from config import CONFIGS


@click.command()
@click.option("--ss-id", "-s", required=True, help="Storage service ID.", type=int)
@click.option("--session-id", "-i", required=True, help="Session descriptor.", type=str)
@click.option(
    "--page", "-p", help="Page (if not set then all AIPs will be fetched).", type=int
)
@click.option(
    "--packages-per-page",
    "-n",
    default=1000,
    help="Packages per page (default 1000).",
    type=int,
)
@click.option("--logfile", "-l", default=None, help="Filepath to log to.", type=str)
@click.option(
    "--lockfile",
    "-o",
    is_flag=True,
    show_default=True,
    default=False,
    help="Use lock file.",
    type=bool,
)
def main(ss_id, session_id, page, packages_per_page, lockfile, logfile):
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s:%(levelname)s:%(name)s:%(message)s",
        filename=logfile,
    )

    logger_name = pathlib.PurePosixPath(sys.argv[0]).name

    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.INFO)

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    # Attempt to acquire lock, if requested
    if lockfile:
        lock_filepath = "/var/lock/aipscan_fetch.lock"
        lock = FileLock(lock_filepath)
        logger.info(f"Using lock file {lock_filepath}.")

        try:
            lock.acquire(timeout=0)
        except Timeout:
            cli.log_and_raise_click_error(logger, "Unable to acquire lock.")

        try:
            fetch_aips(logger, ss_id, session_id, page, packages_per_page, logfile)
        finally:
            lock.release()

    else:
        fetch_aips(logger, ss_id, session_id, page, packages_per_page, logfile)


def fetch_aips(logger, ss_id, session_id, page, packages_per_page, logfile):
    # Initialize Flask app context
    app = cli.create_app_instance(CONFIGS[cli.config_name], db)

    package_list_no = "batch"
    with app.app_context():
        # Get storage service configuration
        storage_service = StorageService.query.get(ss_id)

        if storage_service is None:
            cli.log_and_raise_click_error(
                logger, f"Storage service ID {ss_id} not found."
            )

        # Create a fetch_job record in the AIPscan database and take note of its ID
        datetime_obj_start = datetime.now().replace(microsecond=0)

        fetch_job = database_helpers.create_fetch_job(
            datetime_obj_start, session_id, ss_id
        )
        fetch_job_id = fetch_job.id

        logger.info(f"Created fetch job record {fetch_job_id}.")

        # Create directories, if need be
        packages_dir = fetch.create_packages_directory(session_id)
        fetch.create_mets_directory(session_id)
        create_numbered_subdirs(session_id, package_list_no)

        # Get package data
        packages = fetch.get_packages(storage_service, packages_dir)

        # Determine total packages and start and end package
        total_packages = len(packages["objects"])
        start_item, end_item = fetch.determine_start_and_end_item(
            page, packages_per_page, total_packages
        )

        # Make sure paging window is valid and delete fetch job if not
        if start_item > total_packages:
            db.session.delete(fetch_job)
            db.session.commit()

            message = f"Page {page} would start at package {start_item} but there are only {total_packages} packages. Fetch job deleted."
            cli.log_and_raise_click_error(logger, message)

        logger.info(
            f"Processing packages {start_item} to {end_item} of {total_packages}."
        )

        # Import packages
        processed_packages = fetch.import_packages(
            packages,
            start_item,
            end_item,
            ss_id,
            session_id,
            package_list_no,
            fetch_job_id,
            packages_per_page,
            logger,
        )

        fetch_job = database_helpers.update_fetch_job(
            fetch_job_id, processed_packages, total_packages
        )

        summary = "aips: '{}'; sips: '{}'; dips: '{}'; deleted: '{}'; replicated: '{}'".format(
            fetch_job.total_aips,
            fetch_job.total_sips,
            fetch_job.total_dips,
            fetch_job.total_deleted_aips,
            fetch_job.total_replicas,
        )
        logger.info("%s", summary)
        logger.info(
            f"Updated fetch job record {fetch_job_id} with package type counts."
        )
        logger.info("Processing complete.")


if __name__ == "__main__":
    main()
